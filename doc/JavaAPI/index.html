<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>UM Java API: Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen_api.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">UM Java API
   &#160;<span id="projectnumber">6.12</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('index.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p> 
<img src="infa_logo.png" width="400" height="138" alt="Informatica"/><br/><br/>
<big><b>Ultra Messaging</b></big> <small>(Version 6.12)</small><br/>
<br/><br/><br/>
<center class="mytitle">Ultra Messaging Java API</center>
<center>Copyright (C) 2004-2019, Informatica Corporation.  All Rights Reserved.</center>
<br/><br/><br/>
<h1>Introduction</h1>
</p>
<dl class="section attention"><dt>Attention</dt><dd><b>See the <a href="../DocIntro=en.pdf">Documentation Introduction</a> for important information on copyright, patents, information resources (including Knowledge Base, and How To articles), Marketplace, Support, and other information about Informatica and its products.</b></dd></dl>
<p><br />
 </p>
<h1><a class="anchor" id="javanativeinterfacejni"></a>
Java Native Interface (JNI)</h1>
<p>The UM Java API makes use of the Java Native Interface (JNI) to bridge between the native Java code and the UM library (written in C). This interface provides for two different types of calls: "downcalls" from Java into C, and "upcalls" from C into Java.</p>
<p>While still more expensive (in terms of CPU time) than a call from one Java method to another Java method, downcalls are relatively inexpensive. Upcalls, on the other hand, are significantly more expensive. Our experience (as well as anecdotal evidence from the Internet) indicates that upcalls are 10 to 20 times more expensive than downcalls. This expense translates directly to increased CPU time. In terms of UM, this translates to higher latency and lower throughput.</p>
<p>In addition, an upcall must attach to a Java thread object in the JVM in order to obtain an environment in which to run. Attaching to a new Java thread object is significantly more expensive than re-attaching to an extant Java thread object.</p>
<p>This discussion applies only to receivers. Sending in UM requires mostly downcalls. In fact, performance measurements show that the throughput from a Java sender is generally within a few percentage points of the throughput for a C sender.</p>
<p>Receivers, on the other hand, receive data via callbacks from the UM library into the application code. Callbacks directly equate to JNI upcalls, which (as noted above) are significantly more expensive than JNI downcalls.</p>
<p><br />
 </p>
<h1><a class="anchor" id="garbagecollection"></a>
Garbage Collection</h1>
<p>One significant problem with Java performance is garbage collection. While this makes the programmer's life easier, having your application periodically stop doing application-specific useful work to perform its housekeeping duties significantly degrades the overall performance of the application. This is not to imply that garbage collection is not useful work: in the context of Java, it certainly is. But it does nothing to further the goal of the application itself, namely to receive and process data.</p>
<p>Thus, Java performance is inherently unpredictable and can vary significantly from one instant to the next. As an example, consider the lbmrcv example program supplied with UM. Running the C version will show a fairly steady data rate for each sample printed. Running the Java version will show wildly varying data rates for each sample printed. This is due in large part to the periodic interruption of the application to do garbage collection. See <a class="el" href="index.html#zeroobjectdeliveryzod">Zero Object Delivery (ZOD)</a> for a UM Java feature that reduces the need for garbage collection.</p>
<p><br />
 </p>
<h1><a class="anchor" id="zeroobjectdeliveryzod"></a>
Zero Object Delivery (ZOD)</h1>
<p>UM's Zero Object Delivery (ZOD) feature for Java allows receivers to deliver messages, and sources/receivers deliver events, to an application with no per-message object creation. This lets you write Java sending/receiving applications that require little to no garbage collection at runtime, resulting in lower and more consistent message latencies and hence, better performance.</p>
<p>To benefit from this feature, you must call .dispose() on a message to mark it as available for reuse. To access data from the message when using ZOD, use the following two methods in the <a class="el" href="classcom_1_1latencybusters_1_1lbm_1_1LBMMessage.html">com::latencybusters::lbm::LBMMessage</a> class: </p><ul>
<li>
LBMMessage::dataBuffer </li>
<li>
LBMMessage::dataLength </li>
</ul>
<p>The .dataBuffer() method returns a reference to a thread-local direct ByteBuffer containing the message data. The ByteBuffer's capacity is probably larger than the message data length, so you must call .dataLength() for the actual data length. Discard/ignore the excess data in the ByteBuffer.</p>
<p>See the lbmrcv.java sample applications for examples using these methods.</p>
<p><br />
 </p>
<h1><a class="anchor" id="retainingmessages"></a>
Retaining Messages</h1>
<p>Note that calling LBMMessage.data() is another, valid way to access message data, but this method does create a new byte[] array object. JVM creates the byte[] array object returned by LBMMessage.data() once, on demand, the first time you call .data(), and returns a reference to the same byte[] array object on subsequent calls to .data().</p>
<p>This method is a useful way to keep an LBMMessage object for processing outside of a receiver's callback. The byte[] array object returned by LBMMessage.data() is persistent, that is, the data it contains and any array references remain valid after the receiver's callback returns and until garbage collection. If you need only the message data for further processing, save the return from LBMMessage.data() within the receiver's callback.</p>
<p>If instead you need to promote an entire LBMMessage to a full-fledged independent object for use outside of a receiver's callback, use the LBMMessage.promote() method. This can be beneficial to an application receiving a mix of message types; some requiring additional processing, and others not. receiver callback code can look similar to the following:</p>
<pre class="fragment">if (msg.dataBuffer().getInt() == number_indicating_lots_of_work) {
        /* Promote this message to an object for handoff to a worker thread,
         * so it remains valid after the receiver callback returns. */
        msg.promote();
        workerThread.msgQueue.enqueue(msg);
}
else {
        /* Use ZOD and just read out of msg.dataBuffer(),
         * entirely within the receiver callback. */
        ...
}
</pre><p>LBMMessages promoted to full objects also return their own independent ByteBuffer objects from a call to LBMMessage.dataBuffer(). This means that the ByteBuffer returned from a promoted LBMMessage's.dataBuffer() method is persistent in the same way as the byte[] array object returned from a call to .data() on any LBMMessage object.</p>
<p>See the lbmresp.java sample application for another example of using LBMMessage.promote() to keep a message.</p>
<p><br />
 </p>
<h1><a class="anchor" id="messagesize"></a>
Message Size</h1>
<p>Message size has a significant effect on performance. Smaller messages mean more upcalls are required to receive and process a given amount of data. Larger messages result in fewer upcalls, resulting in better performance. Our tests have shown that smaller messages (64 bytes or smaller) yield approximately 25% of the performance of an equivalent C application, while larger messages (512 bytes and up) can yield 67% of the performance of an equivalent C application.</p>
<p>Granted, the size of the message is beyond the control of the receiving application. But the sender <em>can</em> control the message size. If you can modify the sender, consider using larger (500 bytes or more) messages. If this is not possible, consider blocking multiple "logical" messages together into a single "physical" message. The receiver would then be responsible for deblocking into the constituent messages. Note that this is not the same as batching.</p>
<p>Larger messages also have the advantage of making better use of the network bandwidth. For example, consider a TCP packet. A minimum of 54 bytes (14 for the Ethernet frame, 20 bytes for the IP header, 20 bytes for the TCP header) of overhead are required to send a one-byte message. A 512-byte message requires those same 54 bytes of overhead. On a 100 Mbps Ethernet network, at most 227,272 one-byte messages can be sent each second, resulting in 227,272 bytes of application data. With a 512-byte message, at most 22,084 message can be sent each second, resulting in 11,307,008 bytes of application data.</p>
<p><br />
 </p>
<h1><a class="anchor" id="javaperformance"></a>
Java Performance</h1>
<p>Our experience shows that Java applications are much more sensitive to CPU speed than C applications. In other words, increasing the CPU speed will have more benefit for a Java application than for a C application.</p>
<p>No difference was seen between the 2.4 and 2.6 Linux kernels. In addition, later 2.6 kernels allow the timer frequency to be set to 100 (default), 250, or 1000 Hz. No measurable difference was seen by changing the timer frequency.</p>
<p>Multiple CPUs have a negligible effect in embedded and sequential modes. Using an event queue yields a modest performance increase of approximately 15% over a single CPU. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
