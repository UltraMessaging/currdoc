<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Concepts Guide: Advanced Optimizations</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen_manual.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Concepts Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('advancedoptimizations.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Advanced Optimizations </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The internal design of UM has many compromises between performance and flexibility. For example, there are critical sections which maintain state information which must be kept internally consistent. Since UM allows the application the flexibility of multi-threaded use, those critical sections are protected with Mutex locks. These locks add very little overhead to UM's execution, but "very little" is not the same as "zero". The use of locks is a compromise between efficiency and flexibility. Similar lines of reasoning explain why UM makes use of dynamic memory (malloc and free), and bus-interlocked read/modify/write operations (e.g. atomic increment).</p>
<p>UM provides configuration options which improve efficiency, at the cost of reduced application design flexibility. Application designers who are able to constrain their programs within certain restrictions can take advantage of improved performance and reduced latency outliers (<a class="el" href="umglossary.html#glossaryjitter">jitter</a>).</p>
<p><b>RECEIVE-SIDE</b> </p><ul>
<li>
<a class="el" href="advancedoptimizations.html#receivethreadbusywaiting">Receive Thread Busy Waiting</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#receivebufferrecycling">Receive Buffer Recycling</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#singlereceivingthread">Single Receiving Thread</a> - not generally applicable. </li>
<li>
<a class="el" href="advancedoptimizations.html#lbmcontextprocesseventsex">Extended Context Process Events</a> - not generally applicable. </li>
<li>
<a class="el" href="advancedoptimizations.html#receivemultipledatagrams">Receive Multiple Datagrams</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#transportdemultiplexertablesize">Transport Demultiplexer Table Size</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#xsplatencyreduction">XSP Latency Reduction</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#receivesidebatching">Receive-Side Batching</a> - for use with event queues. </li>
</ul>
<p><b>SEND-SIDE</b> </p><ul>
<li>
<a class="el" href="advancedoptimizations.html#smartsources">Smart Sources</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#zerocopysendapi">Zero-Copy Send API</a> (not recommended; see <a class="el" href="advancedoptimizations.html#comparisonofzerocopyandsmartsources">Comparison of Zero Copy and Smart Sources</a>). </li>
</ul>
<p><b>GENERAL (both receivers and senders)</b> </p><ul>
<li>
<a class="el" href="advancedoptimizations.html#corepinning">Core Pinning</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#memorylatencyreduction">Memory Latency Reduction</a>. </li>
<li>
<a class="el" href="advancedoptimizations.html#zeroobjectdelivery">Zero Object Delivery</a> (Java and .NET). </li>
</ul>
<p><br />
 </p>
<h1><a class="anchor" id="receivethreadbusywaiting"></a>
Receive Thread Busy Waiting&nbsp;&nbsp;<small><a href="#receivethreadbusywaiting">&lt;-</a></small></h1>
<p>Busy looping is a method for reducing latency and especially latency outliers (<a class="el" href="umglossary.html#glossaryjitter">jitter</a>) by preventing threads from going to sleep. In an event-driven system, if a thread goes to sleep waiting for an event, and the event happens, the operating system needs to wake the thread back up and schedule its execution on a CPU core. This can take several microseconds. Alternatively, if the thread avoids going to sleep and "polls" for the event in a tight loop, it will detect and be able to start processing the event without the operating system scheduling delay.</p>
<p>However, remember that a thread that never goes to sleep will fully consume a CPU core. If you have more busy threads than you have CPU cores in your computer, you can have CPU thrashing, where threads are forced to time-share the cores. This can produce <em>worse</em> latency than sleep-based waits.</p>
<p>Only use busy waiting if there are enough cores to allocate a core exclusively to each busy thread. Also, <a class="el" href="umglossary.html#glossarypinning">pinning</a> threads to cores is highly recommended to prevent thread migration across cores, which can introduce latency and significant jitter.</p>
<p><br />
 </p>
<h2><a class="anchor" id="networksocketbusywaiting"></a>
Network Socket Busy Waiting&nbsp;&nbsp;<small><a href="#networksocketbusywaiting">&lt;-</a></small></h2>
<p>The UM receive code performs socket calls to wait for network events, like received data. By default, UM does sleep-based waiting for events. For example, if there are no packets waiting to be read from any of the sockets, the operating system will put the receive thread to sleep until a packet is received.</p>
<p>However, the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#filedescriptormanagementbehaviorcontext">file_descriptor_management_behavior (context)</a> configuration option can be used to change the behavior of the receive thread to <em>not</em> sleep. Instead, the socket is checked repeatedly in a tight loop - busy waiting. With most use cases, enabling "busy wait" will typically reduce <em>average</em> latency only a little, but it can significantly reduce latency outliers (<a class="el" href="umglossary.html#glossaryjitter">jitter</a>).</p>
<p>For network-based transports, a receive thread can either be the main context thread, or it can be an <a class="el" href="umfeatures.html#transportservicesproviderxsp">XSP thread</a>. A given application can have more than one context, and a given context can have zero or more XSPs. The threads of each context and XSP can be independently configured to have either busy waiting or sleep waiting.</p>
<p>Note that when creating an XSP, it is not unusual to simply let the XSP inherit the parent context's attributes. However, a common XSP use case is to create a single XSP for user data, and leave the parent context for Topic Resolution and other overhead. In this case, you may want to configure the parent context to use sleep-based waiting ("pend"), and configure the XSP to use busy waiting ("busy-wait"). You will need to pass a context attribute to the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ae38035c6036a84f07f9ba7cee1747dbf">lbm_xsp_create()</a> API.</p>
<p><b>A Better Alternative</b></p>
<p>Kernel bypass network drivers typically have a busy waiting mode of operation which happens inside the driver itself. For example Solarflare's Onload driver can be configured to do busy waiting. This can produce even greater improvement than UM receive thread busy waiting. When using a busy waiting kernel bypass network driver like Onload, the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#filedescriptormanagementbehaviorcontext">file_descriptor_management_behavior (context)</a> configuration option should be left at its default, "pend".</p>
<p><br />
 </p>
<h2><a class="anchor" id="ipctransportbusywaiting"></a>
IPC Transport Busy Waiting&nbsp;&nbsp;<small><a href="#ipctransportbusywaiting">&lt;-</a></small></h2>
<p>The <a class="el" href="transporttypes.html#transportlbtipc">Transport LBT-IPC</a> does not use the context or XSP threads for receiving messages. It has its own internal thread which can be configured for busy waiting with the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtipcoperation.html#transportlbtipcreceiverthreadbehaviorcontext">transport_lbtipc_receiver_thread_behavior (context)</a> option.</p>
<p><br />
 </p>
<h2><a class="anchor" id="smxtransportbusywaiting"></a>
SMX Transport Busy Waiting&nbsp;&nbsp;<small><a href="#smxtransportbusywaiting">&lt;-</a></small></h2>
<p>The <a class="el" href="transporttypes.html#transportlbtsmx">Transport LBT-SMX</a> does not use the context or XSP threads for receiving messages. It has its own internal thread which always operates in busy waiting. I.e. it cannot be configured to sleep waiting.</p>
<p><br />
 </p>
<h1><a class="anchor" id="zeroobjectdelivery"></a>
Zero Object Delivery&nbsp;&nbsp;<small><a href="#zeroobjectdelivery">&lt;-</a></small></h1>
<p>Zero Object Delivery is a set of Java and .NET programming conventions to avoid garbage.</p>
<p>In Java and .NET, garbage collection (GC) can be a very useful feature to simplify programming. However, for low latency applications, GC is a problem. Most JVMs impose milliseconds of latency when GC runs. Also, unnecessary object creation introduces additional overhead.</p>
<p>Most high-performance Java and .NET programs avoid GC by saving objects that are no longer needed and reusing them. Most of this is the responsibility of the application programmer. However, UM application callbacks represent a special case. When UM needs to deliver an event, like a received message, to a Java or .NET program, it typically passes to the callback one or more objects containing the pertinent event data. UM needs to know when the application is finished with those passed-in objects so that UM can re-use them.</p>
<p>This requires the application to follow a set of conventions. Applications that deviate from these conventions will typically suffer from higher latency due to unnecessary object creation and garbage collection. These programming conventions are collectively called "Zero
Object Delivery" (ZOD).</p>
<p>The ZOD programming convention are the same between Java and .NET. (ZOD does not apply to the C API.)</p>
<ol>
<li>
Special data access methods must be used to access the received message's data. </li>
<li>
If a message needs to be processed outside of the application receiver callback, it must be "promoted". A UM recycler must be used to ensure no garbage is created. </li>
<li>
Every message must be explicitly deleted when the application is finished processing it. This is true for messages fully processed by the receiver callback, and for promoted messages that are processed outside of the receiver callback. This is done by calling the "dispose()" method. </li>
</ol>
<p>Details of these coding conventions can be found at: </p><ul>
<li>
<a class="el" href="applicationdesignprinciples.html#javamessagereception">Java Message Reception</a> </li>
<li>
<a class="el" href="applicationdesignprinciples.html#netmessagereception">.NET Message Reception</a> </li>
</ul>
<dl class="section note"><dt>Note</dt><dd>While the coding conventions of ZOD are most often seen with the receiver callback, many of the same conventions exist with other event delivery callbacks. For example, source events should be disposed. Source events must also be promoted before they are passed to another thread for processing, after which they should be disposed and recycled.</dd></dl>
<p><br />
 </p>
<h1><a class="anchor" id="receivebufferrecycling"></a>
Receive Buffer Recycling&nbsp;&nbsp;<small><a href="#receivebufferrecycling">&lt;-</a></small></h1>
<p>By default, the UM receive code base allocates a fresh buffer for each received datagram. This allows the user a great degree of threading and buffer utilization flexibility in the application design.</p>
<p>For transport types RM (reliable multicast), RU (Reliable Unicast), and IPC (shared memory), you can set a configuration option to enable reuse of receive buffers, which can avoid per-message dynamic memory calls (malloc/free). This produces a modest reduction in average latency, but more importantly, can significantly reduce occasional latency outliers (<a class="el" href="umglossary.html#glossaryjitter">jitter</a>).</p>
<p>See the configuration options: </p><ul>
<li>
RM - <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtrmoperation.html#transportlbtrmrecyclereceivebufferscontext">transport_lbtrm_recycle_receive_buffers (context)</a> </li>
<li>
RU - <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtruoperation.html#transportlbtrurecyclereceivebufferscontext">transport_lbtru_recycle_receive_buffers (context)</a> </li>
<li>
IPC - <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtipcoperation.html#transportlbtipcrecyclereceivebufferscontext">transport_lbtipc_recycle_receive_buffers (context)</a> </li>
</ul>
<p>Note that setting the option does not guarantee the elimination of per-message malloc and free except in fairly restrictive use cases.</p>
<p>Also note that this feature is different from the Java and .NET ZOD recycler. See <a class="el" href="advancedoptimizations.html#zeroobjectdelivery">Zero Object Delivery</a>.</p>
<p><br />
 </p>
<h2><a class="anchor" id="receivebufferrecyclingrestrictions"></a>
Receive Buffer Recycling Restrictions&nbsp;&nbsp;<small><a href="#receivebufferrecyclingrestrictions">&lt;-</a></small></h2>
<p>There are no hard restrictions to enabling buffer recycling. I.e. it is not functionally not compatible with any use patterns or UM features. However, some use patterns will prevent the recycling of the receive buffer, and therefore not deliver the benefit, even if the configuration option is set.</p>
<ul>
<li>
<b><a class="el" href="umobjects.html#eventqueueobject">Event Queues</a></b> - Event Queues prevent the recycling of receive buffers. When the UM library transfers a received message to an event queue for later processing, it allocates (malloc) a new message receive buffer. </li>
<li>
<b><a class="el" href="umobjects.html#messageobjectretention">Message Object Retention</a></b> - Message retention prevents the recycling. For context-thread receive message callbacks, the act of retaining a message allocates (mallocs) a new message receive buffer. </li>
<li>
<b><a class="el" href="fundamentalconcepts.html#persistence">Persistence</a></b> - For a persistent receiver, enabling receive buffer recycling will <em>reduce</em> dynamic memory usage (malloc/free), but does not eliminate it. Certain persistence-related features require the use of dynamic memory. </li>
<li>
<b><a class="el" href="packetloss.html">Packet Loss</a></b> - Applications typically use <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#ordereddeliveryreceiver">Ordered Delivery</a>. When packets are lost, UM needs to internally retain newly received messages so that they can be delivered after the missing messages are retransmitted. This internal retention prevents the newly received message buffers from being recycled. </li>
<li>
<b><a class="el" href="architecture.html#messagefragmentationandreassembly">Message Fragmentation and Reassembly</a></b> - Large application messages must be split into smaller fragments and sent serially. The receiver must internally retain these fragments so that the original large message can be reassembled and delivered to the application. This internal retention prevents the fragment message buffers from being recycled. </li>
</ul>
<p>Note that in spite of the restrictions that can prevent recycling of receive message buffers, UM dynamically takes advantage of recycling as much as it can. E.g. if there is a loss event which suppresses recycling while waiting for retransmission, once the gap is filled and pending messages are delivered, UM will once again be able to recycle its receive buffers.</p>
<p>Of specific interest for <em>persistent</em> receivers is the use of <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/designingpersistentreceivers.html#explicitacknowledgments">Explicit Acknowledgments</a>, either to batch ACKs, or simply defer them. Instead of retaining the messages, which prevents message buffer recycling, you can extract the ACK information from a message and allow the return from the receiver callback to delete and recycle the message buffer without acknowledging it.</p>
<p>See <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/designingpersistentreceivers.html#objectfreeexplicitacknowledgments">Object-free Explicit Acknowledgments</a> for details.</p>
<p><br />
 </p>
<h1><a class="anchor" id="singlereceivingthread"></a>
Single Receiving Thread&nbsp;&nbsp;<small><a href="#singlereceivingthread">&lt;-</a></small></h1>
<p>This feature optimizes the execution of UM receive-path code by converting certain thread-safe operations to more-efficient thread-unsafe operations. For example, certain bus-locked operations (e.g. atomic increment) are replaced by non-bus-locked equivalents (e.g. non-atomic increment). This can reduce the latency of delivering received messages to the application, but does so at the expense of thread safety.</p>
<p>This feature is often used in conjunction with the <a class="el" href="advancedoptimizations.html#contextlockreduction">Context Lock Reduction</a> feature.</p>
<p>The <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#transportsessionsinglereceivingthreadcontext">transport_session_single_receiving_thread (context)</a> configuration option enables this feature.</p>
<p>Except as listed in the restrictions below, the Single Receiving Thread feature should be compatible with all other receive-side UM features.</p>
<p><br />
 </p>
<h2><a class="anchor" id="singlereceivingthreadrestrictions"></a>
Single Receiving Thread Restrictions&nbsp;&nbsp;<small><a href="#singlereceivingthreadrestrictions">&lt;-</a></small></h2>
<p>It is very important for applications using this feature to be designed within certain restrictions.</p>
<ul>
<li>
<p class="startli"><b>Threading</b> - The intended use case is for each received message to be fully processed by the UM thread that delivers the message to the application. Note that the <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a> feature <em>is</em> compatible with the Single Receiving Thread feature.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No <a class="el" href="umobjects.html#eventqueueobject">Event Queues</a></b> - Event queues cannot be used with Single Receiving Thread.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b><a class="el" href="umobjects.html#messageobjectretention">Message Object Retention</a></b> - Most traditional uses of message retention are related to giving a message to an alternate thread for processing. This is not compatible with Single Receiving Thread feature.</p>
<p>However, there are some use cases where message retention is viable when used with Single Receiving Thread: when a message must be held for <em>future</em> processing, and that processing will be done by the same thread.</p>
<p>For example, a persistent application might use <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/designingpersistentreceivers.html#explicitacknowledgments">Explicit Acknowledgments</a> to delay message acknowledgement until the application completes a handshake with a remote service. As long as it is the same thread which initially receives and retains the message as that which completes the explicit acknowledgement of the message, it is supported to use message retain / message delete.</p>
<dl class="section note"><dt>Note</dt><dd>If the <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a> feature is used, care must be taken to ensure that the same XSP thread is used to perform all processing for a received message. I.e. a different XSP or the main context may not be used to complete processing on a deferred retained message. For example, a user-scheduled timer event will be delivered using the main context thread, and therefore cannot complete processing of a retained message.</dd></dl>
</li>
<li>
<b>Transport Type</b> - The Single Receiving Thread feature does not enhance the operation of <a class="el" href="transporttypes.html#transportbroker">Broker</a> or <a class="el" href="transporttypes.html#transportlbtsmx">SMX</a> transport types. These transport types use somewhat different internal buffer handling. Note that these transport types are technically compatible with the Single Receiving Thread feature, they just don't benefit from it. </li>
</ul>
<p><br />
 </p>
<h1><a class="anchor" id="lbmcontextprocesseventsex"></a>
Extended Context Process Events&nbsp;&nbsp;<small><a href="#lbmcontextprocesseventsex">&lt;-</a></small></h1>
<p>Most developers of UM applications use a multi-threaded approach to their application design. For example, they typically have one or more application threads, and they create a UM context with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#operationalmodecontext">embedded mode</a>, which creates a separate <a class="el" href="umobjects.html#contextobject">context thread</a>.</p>
<p>However, there is a model of application design in which a single thread is used for the entire application. In this case, the UM context must be created with <a class="el" href="architecture.html#sequentialmode">Sequential Mode</a> and the application must regularly call the UM event processor API, usually with the <code>msec</code> parameter set to zero. In this design, there is no possibility that application code, UM API code, and/or UM context code will be executing concurrently.</p>
<p>The <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a> API allows the application to enable specialized optimizations. (For Java and .NET use the context object's <code>"processEvents()"</code> method with 2 or more input parameters. See <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/javaapi.tag:../JavaAPI/" href="../JavaAPI/classcom_1_1latencybusters_1_1lbm_1_1LBMContext.html#a78125f80281b91950f0409a4caddc4e3">com::latencybusters::lbm::LBMContext::processEvents</a>.)</p>
<p><br />
 </p>
<h2><a class="anchor" id="contextlockreduction"></a>
Context Lock Reduction&nbsp;&nbsp;<small><a href="#contextlockreduction">&lt;-</a></small></h2>
<p>The application can improve performance by suppressing the taking of certain mutex locks within the UM context processing code. This can reduce the latency of delivering received messages to the application, but does so at the expense of thread safety.</p>
<p>This feature is often used in conjunction with the <a class="el" href="advancedoptimizations.html#singlereceivingthread">Single Receiving Thread</a> feature.</p>
<dl class="section warning"><dt>Warning</dt><dd>It is very important for the application to ensure that UM code related to a given context cannot be executed concurrently by multiple threads when this feature is used. This includes UM object creation and send-path API functions. I.e. the application may not call a UM message send API by one thread while another thread is calling <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a>. However, it is permissible for a context thread callback to call a UM message send API, within the restrictions of the send API being used.</dd></dl>
<p>To enable this feature, call <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a>, passing in the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#af09539ed3d358e071a3a5b0a903335f0">lbm_process_events_info_t</a> structure with the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a4d3385918ddb747b734f68151ff00d04">LBM_PROC_EVENT_EX_FLAG_NO_MAIN_LOOP_MUTEX</a> bit set in the <code>flags</code> field. (<a class="el" href="architecture.html#sequentialmode">Sequential Mode</a> is required for this feature.)</p>
<p><br />
 </p>
<h2><a class="anchor" id="contextlockreductionrestrictions"></a>
Context Lock Reduction Restrictions&nbsp;&nbsp;<small><a href="#contextlockreductionrestrictions">&lt;-</a></small></h2>
<p>It is very important for applications using this feature to be designed within certain restrictions.</p>
<ul>
<li>
<p class="startli"><b>Threading</b> - It is critical that Context Lock Reduction be used <em>only</em> if <a class="el" href="architecture.html#sequentialmode">Sequential Mode</a> is used and there is no possibility of concurrent execution of UM code for a given context.</p>
<p>It is further strongly advised that the same thread be used for all UM execution within a given context. I.e. it is not guaranteed to be safe if the application has multiple threads which can operate on a context, even if the application guarantees that only one thread at a time will execute the UM code.</p>
<p>Note that if an application maintains two contexts, it is acceptable for a different thread to be used to operate on each context. However, it is not supported to pass UM objects between the threads.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a></b> - The Context Lock Reduction feature is not compatible with XSP.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No <a class="el" href="umobjects.html#eventqueueobject">Event Queues</a></b> - Event queues cannot be used with Context Lock Reduction.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No <a class="el" href="transporttypes.html#transportlbtsmx">SMX</a> or <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportacceleration.html#myricomdatagrambypasslayerdbl">DBL</a></b> - Context Lock Reduction is not compatible with SMX or DBL transports. This is because these transports create independent threads to monitor their respective transport types.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b><a class="el" href="transporttypes.html#transportlbtipc">Transport LBT-IPC</a></b> - Context Lock Reduction was not designed with the IPC transport in mind. By default, IPC creates an independent thread to monitor the shared memory, which is not compatible with Context Lock Reduction. However, in principle, it is possible to specify that the IPC receiver should use sequential mode (see <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtipcoperation.html#transportlbtipcreceiveroperationalmodecontext">transport_lbtipc_receiver_operational_mode (context)</a>), and then write your application to use the same thread to call the context and IPC event processing APIs. However, be aware that the IPC event processing API does not have an extended form, so IPC will simply continue to take the locks it is designed to take.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b><a class="el" href="umobjects.html#messageobjectretention">Message Object Retention</a></b> - Most traditional uses of <a class="el" href="umobjects.html#messageobjectretention">Message Object Retention</a> are related to handing a message to an alternate thread for processing. This is not compatible with Context Lock Reduction because the alternate thread is responsible for deleting the message when it is done. This represents two threads making API calls for the same context, which is not allowed for the Context Lock Reduction feature.</p>
<p>However, there are some use cases where message retention is viable when used with Context Lock Reduction: when a message must be held for <em>future</em> processing, and that processing will be done by the same thread.</p>
<p>For example, a persistent application might use <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/designingpersistentreceivers.html#explicitacknowledgments">Explicit Acknowledgments</a> to delay message acknowledgement until the application completes a handshake with a remote service. As long as it is the same thread which initially receives and retains the message as that which completes the explicit acknowledgement of the message, it is supported to use message retain / message delete.</p>
<p class="endli"></p>
</li>
<li>
<b>No <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a41a6bc8b3a5d8670b4066cf14fbe2556">LBM_SRC_BLOCK</a></b> - All forms of UM send message must be done non-blocking (i.e. with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ab8a470f02029480f179cc4872b7fa713">LBM_SRC_NONBLOCK</a>). This is because of the way UM blocks calls that cannot be completed; the context thread explicitly wakes up the blocked call when appropriate. But if the same thread is being used to run the context (via the process events API) and also sending messages, a blocked send call will never be woken up. </li>
</ul>
<p><br />
 </p>
<h2><a class="anchor" id="gettimeofdayreduction"></a>
Gettimeofday Reduction&nbsp;&nbsp;<small><a href="#gettimeofdayreduction">&lt;-</a></small></h2>
<p>UM's main context loop calls gettimeofday() in strategic places to ensure that its internal timers are processed correctly. However, there is a "polling" model of application design in which <a class="el" href="architecture.html#sequentialmode">Sequential Mode</a> is enabled and the context event processing API is called in a fast loop with the <code>msec</code> parameter set to zero. This results in the internal context call to gettimeofday() to happen unnecessarily frequently.</p>
<p>A polling application can improve performance by suppressing the internal context calls to gettimeofday(). This can reduce the latency of delivering received messages to the application.</p>
<p>To enable this feature, call <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a>, passing in the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#af09539ed3d358e071a3a5b0a903335f0">lbm_process_events_info_t</a> structure with the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ad38d38d8b34bb913ac48940f6bc52a94">LBM_PROC_EVENT_EX_FLAG_USER_TIME</a> bit set in the <code>flags</code> field. In addition, the application must set the <code>time_val</code> field in <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#af09539ed3d358e071a3a5b0a903335f0">lbm_process_events_info_t</a> with the value returned by gettimeofday(). (<a class="el" href="architecture.html#sequentialmode">Sequential Mode</a> is required for this feature.)</p>
<dl class="section note"><dt>Note</dt><dd>The internal UM timers generally use millisecond precision. Users of the gettimeofday() reduction feature typically design their application to fetch a new value for <code>time_val</code> only a few times per millisecond.</dd></dl>
<p><br />
 </p>
<h2><a class="anchor" id="gettimeofdayreductionrestrictions"></a>
Gettimeofday Reduction Restrictions&nbsp;&nbsp;<small><a href="#gettimeofdayreductionrestrictions">&lt;-</a></small></h2>
<ul>
<li>
<b>Monotonically Increasing Time</b> - The application is responsible for ensuring that each call to <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a> has a <code>time_val</code> field value which is greater than or equal to the previous <code>time_val</code>. </li>
<li>
<b><code>"msec"</code> must be 0</b> - You must call <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a> with the <code>"msec"</code> parameter set to zero. You can't tell <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a55b77c3057028116314fe5ff8e6afb25">lbm_context_process_events_ex()</a> to loop for a period of time, and also tell it not to call gettimeofday(); UM won't see the passage of time. </li>
</ul>
<p><br />
 </p>
<h1><a class="anchor" id="receivemultipledatagrams"></a>
Receive Multiple Datagrams&nbsp;&nbsp;<small><a href="#receivemultipledatagrams">&lt;-</a></small></h1>
<p>A UM receiver for UDP-based protocols normally retrieves a single UDP datagram from the socket with each socket read. Setting <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#multiplereceivemaximumdatagramscontext">multiple_receive_maximum_datagrams (context)</a> to a value greater than zero directs UM to retrieve up to that many datagrams with each socket read. When receive socket buffers accumulate multiple messages, this feature improves CPU efficiency, which reduces the probability of loss, and also reduces total latency for those buffered datagrams. Note that UM does not need to wait for that many datagrams to be received before processing them; if fewer datagrams are in the socket's receive buffer, only the available datagrams are retrieved.</p>
<p>The <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#multiplereceivemaximumdatagramscontext">multiple_receive_maximum_datagrams (context)</a> configuration option defaults to 0 so as to retain previous behavior, but users are encouraged to set this to a value between 2 and 10. (Having too large a value during a period of overload can allow starvation of low-rate Transport Sessions by high-rate Transport Sessions.)</p>
<p>Note that in addition to increasing efficiency, setting <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#multiplereceivemaximumdatagramscontext">multiple_receive_maximum_datagrams (context)</a> greater than one can produce changes in the dynamic behavior across multiple sockets. For example, let's say that a receiver is subscribed to two Transport Sessions, A and B. Let's further say that Transport Session A is sending message relatively quickly and has built up several datagrams in its socket buffer. But in this scenario, B is sending slowly. If <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#multiplereceivemaximumdatagramscontext">multiple_receive_maximum_datagrams (context)</a> is zero or one, the two sockets will compete equally for UM's attention. I.e. B's socket will still have a chance to be read after each A datagram is read and processed.</p>
<p>However, if <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#multiplereceivemaximumdatagramscontext">multiple_receive_maximum_datagrams (context)</a> is 10, then UM can process up to 10 of A's messages before giving B a chance to be read. This is desirable if low message latency is equally important across all Transport Sessions; the efficiency improvement derived by retrieving multiple datagrams with each read operation results in lower overall latency. However, if different transport sessions' data have different priorities in terms of latency, then processing 10 messages of a low priority transport session can unnecessarily delay processing of messages from a higher priority transport session.</p>
<p>In this case, the <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a> feature can be used to prioritize different transport sessions differently and prevent low-priority messages from delaying high-priority messages.</p>
<p>Note that UM versions prior to 6.13 could see occasional increases in latency outliers when this feature was used. As of UM version 6.13, those outliers have been fixed (see <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/changelog.tag:../ChangeLog/" href="../ChangeLog/umversion6_13.html#bug10726">bug10726</a>).</p>
<p><br />
 </p>
<h2><a class="anchor" id="receivemultipledatagramscompatibility"></a>
Receive Multiple Datagrams Compatibility&nbsp;&nbsp;<small><a href="#receivemultipledatagramscompatibility">&lt;-</a></small></h2>
<p>The Receive Multiple Datagrams feature modifies the behavior of the UDP-based transport protocols: LBT-RM and LBT-RU.</p>
<p>(Note: prior to UM version 6.13, the Receive Multiple Datagrams feature also affected MIM and UDP-based Topic Resolution. But this could introduced undesired latencies, so starting with UM 6.13, MIM and Topic Resolution no longer use Receive Multiple Datagrams.)</p>
<p><br />
 </p>
<h2><a class="anchor" id="receivemultipledatagramsrestrictions"></a>
Receive Multiple Datagrams Restrictions&nbsp;&nbsp;<small><a href="#receivemultipledatagramsrestrictions">&lt;-</a></small></h2>
<p>The Receive Multiple Datagrams feature does not affect the following UM features:</p>
<ul>
<li>
Non-UDP Transport Protocols (TCP, IPC, SMX). </li>
<li>
MIM (as of UM version 6.13). </li>
<li>
UDP-based Topic Resolution (as of UM version 6.13). </li>
<li>
All TCP-based features (Unicast Immediate Message, Late Join, Persistent Store Recovery, UM Response messages). </li>
<li>
Non-Linux. </li>
<li>
Linux prior to kernel version 2.6.33, and glibc in version 2.12 (released in May, 2010). </li>
</ul>
<p><br />
 </p>
<h1><a class="anchor" id="transportdemultiplexertablesize"></a>
Transport Demultiplexer Table Size&nbsp;&nbsp;<small><a href="#transportdemultiplexertablesize">&lt;-</a></small></h1>
<p>A UM <a class="el" href="fundamentalconcepts.html#transportsessions">Transport Session</a> can have multiple sources (topics) mapped to it. For example, if a publishing application creates two sources with the same multicast address and destination port, both sources will be carried on the same transport session. A receiver joined to that transport session must separate (demultiplex) the topics, either for delivery to the proper receiver callback, or for discarding.</p>
<p>The demultiplexing of the topics is managed by a hash table (not the same kind of hash table that manages the Topic Resolution cache). As a result, the processing of received messages can be made more efficient by optimally sizing the hash table. This is done using the configuration option <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#transportdemuxtableszreceiver">transport_demux_tablesz (receiver)</a>.</p>
<p>Unlike many hash tables, the transport demultiplexer needs to have a number of buckets which is a power of two. The demultiplexing code will be most efficient if the number of buckets is equal to or greater than the number of sources created on the transport session. In that case, the hash function is "perfect", which is to say that there will never be any collisions. Note that if the number of buckets is smaller than the number of sources, the collision resolution process is O(log N) where N is the length of the collision chain.</p>
<p>The only disadvantage of increasing the size of the hash table is memory usage (each bucket is 16 bytes on 64-bit architectures). Having a larger than optimal table does not make performance worse.</p>
<p>Note that if the number of sources is small, only a small degree of efficiency improvement results from optimally sizing the hash table.</p>
<p><br />
 </p>
<h1><a class="anchor" id="smartsources"></a>
Smart Sources&nbsp;&nbsp;<small><a href="#smartsources">&lt;-</a></small></h1>
<p>The normal <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a91f4b9cb04fe1323ec56833211cc5cb7">lbm_src_send()</a> function (and its Java and .NET equivalents) are very flexible and support the full range of UM's rich feature set. To provide this level of capability, it is necessary to make use of dynamic (malloc/free) memory, and critical section locking (mutex) in the send path. While modern memory managers and thread locks are very efficient, they do introduce some degree of variability of execution time, leading to latency outliers (<a class="el" href="umglossary.html#glossaryjitter">jitter</a>) potentially in the millisecond range.</p>
<p>For applications which require even higher speed and very consistent timing, and are able to run within certain constraints, UM has an alternate send feature called Smart Source. This is a highly-optimized send path with no dynamic memory operations or locking; all allocations are done at source creation time, and lockless algorithms are used throughout. To achieve these improvements, Smart Source imposes a number of restrictions (see <a class="el" href="advancedoptimizations.html#smartsourcerestrictions">Smart Sources Restrictions</a>).</p>
<p>The Smart Source feature provides the greatest benefit when used in conjunction with a <a class="el" href="umglossary.html#glossarykernelbypass">kernel bypass</a> network driver.</p>
<dl class="section note"><dt>Note</dt><dd>the Smart Source feature is <em>not</em> the same thing as the <a class="el" href="advancedoptimizations.html#zerocopysendapi">Zero-Copy Send API</a> feature; see <a class="el" href="advancedoptimizations.html#comparisonofzerocopyandsmartsources">Comparison of Zero Copy and Smart Sources</a>.</dd></dl>
<p>One design feature that is central to Smart Sources is the pre-allocation of a fixed number of carefully-sized buffers during source creation. This allows deterministic algorithms to be used for the management of message buffers throughout the send process. To gain the greatest benefit from Smart Sources, the application builds its outgoing messages directly in one of the pre-allocated buffers and submits the buffer to be sent.</p>
<p>To use Smart Sources, a user application typically performs the following steps:</p>
<ol>
<li>
Create a context with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a8058947690bd0995bc2c59d4a61b462f">lbm_context_create()</a>, as normal. </li>
<li>
Create the topic object and the Smart Source with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a1ba60407fa2bde0997aab6d5a5d2da1a">lbm_src_topic_alloc()</a> and <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a9a8d169a8c2a90b8442d4eb3e3711c9c">lbm_ssrc_create()</a>, respectively. Use <a class="el" href="advancedoptimizations.html#smartsourceconfiguration">Smart Sources Configuration</a> to pre-allocate the desired number of buffers. </li>
<li>
Get the desired number of messages buffers with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a98c5450e9fb93f30bd80fa33a2380dfc">lbm_ssrc_buff_get()</a> and initialize them if desired. The application typically constructs outgoing messages directly in these buffers for transmission. </li>
<li>
Send messages with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a2f50f56536778332e9f5712b06546425">lbm_ssrc_send_ex()</a>. The buffers gotten in the previous step must be used. </li>
<li>
While most applications manage the message buffers internally, it is also possible to give the buffers back to UM with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ab57120414bf92bd2f44de7aea97e3f1b">lbm_ssrc_buff_put()</a>, and then getting them again for subsequent sends. Getting and putting messages buffers can simplify application design at the expense of extra overhead. </li>
<li>
To clean up, delete the Smart Source with <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a9cd9dfee7a9c268059084a3c327f65f9">lbm_ssrc_delete()</a>. It is not necessary to "put" the message buffers back to UM; they will be freed automatically when the Smart Source is deleted. </li>
</ol>
<p>For details, see the example applications <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/example.tag:../example/" href="../example/index.html#examplelbmssrc_c">Example lbmssrc.c</a> or <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/java_example.tag:../java_example/" href="../java_example/index.html#examplelbmssrc_java">Example lbmssrc.java</a>.</p>
<dl class="section warning"><dt>Warning</dt><dd>To avoid the overhead of locking, the Smart Source API functions are not thread-safe. Applications must be written to avoid concurrent calls. In particular, the application is restricted to sending messages on a given <a class="el" href="fundamentalconcepts.html#transportsessions">Transport Session</a> with one thread. If <a class="el" href="advancedoptimizations.html#smartsourcedefensivechecks">Smart Source Defensive Checks</a> are enabled, the first call to send a message on a newly-created Transport Session captures the ID of the calling thread. Subsequently, only that thread is allowed to call send for Smart Sources on that Transport Session. For applications which have multiple sending threads, Smart Source topics must be mapped to Transport Sessions carefully such that all of the topics on a given Transport Session are managed by the same sending thread.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>There are no special requirements on the receive side when using Smart Sources. Normal receiving code is used.</dd></dl>
<p><br />
 </p>
<h2><a class="anchor" id="smartsourcemessagebuffers"></a>
Smart Source Message Buffers&nbsp;&nbsp;<small><a href="#smartsourcemessagebuffers">&lt;-</a></small></h2>
<p>When a Smart Source is created, UM pre-allocates a set of user buffers according to the configuration options <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a> and <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcuserbuffercountsource">smart_src_user_buffer_count (source)</a>.</p>
<p>As of UM version 6.12, Smart Source supports <a class="el" href="umglossary.html#glossaryumfragmentation">UM fragmentation</a>. Which is to say that messages larger than the transport's <a class="el" href="architecture.html#datagrammaxsizes">Datagram Max Sizes</a> can be sent, which the Smart Source will split into multiple datagrams.</p>
<p>For example, an application can configure <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a> to be 2000, while the datagram max size is set to 1500 (network MTU size). During operation, the application might send a 500-byte message. This will not require any fragmentation; the message is sent in a single network packet. However, when the application sends a 2000-byte message, the Smart Source will split it into two datagrams. This avoids IP fragmentation. The precise sizes of those datagrams will depend on the space reserved for headers, and is subject to change with different versions of UM.</p>
<p>Another feature available as of UM version 6.12 is the user-specified buffer. This allows an application to send messages larger than the configured <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a>. Instead of building the message in a pre-allocated Smart Source buffer, the application must allocate and manage its own user-supplied buffer. To use this feature, the application supplies both a pre-allocated buffer and a user-supplied buffer. The Smart Source will use the supplied pre-allocated buffer as a "work area" for building the datagram with proper headers, and use the user-supplied buffer for message content.</p>
<p>For example to use the buffer <code>"ubuffer"</code>, you simply set the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a366727d0b5f3039e03b9c4bce8065da5">LBM_SSRC_SEND_EX_FLAG_USER_SUPPLIED_BUFFER</a> flag and the <code>usr_supplied_buffer</code> field in the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a33886eeac2e67170effea009d2dc3a35">lbm_ssrc_send_ex_info_t</a> passed to the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a2f50f56536778332e9f5712b06546425">lbm_ssrc_send_ex()</a> API function, as shown below: </p><div class="fragment"><div class="line"><span class="keywordtype">char</span> *ubuffer = malloc(65536);  <span class="comment">/* Large user-supplied buffer. */</span></div><div class="line"><a class="codeRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/structlbm__ssrc__send__ex__info__t__stct.html">lbm_ssrc_send_ex_info_t</a> info;</div><div class="line">info.<a class="codeRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/structlbm__ssrc__send__ex__info__t__stct.html#a20b6feb8a3737d2ad4330c64552ba18c">flags</a> = 0;</div><div class="line"><span class="keywordtype">char</span> *ss_buffer = NULL;  <span class="comment">/* Smart Source pre-allocated buffer. */</span></div><div class="line">...</div><div class="line">lbm_ssrc_buff_get(ssrc, &amp;ss_buffer, 0);  <span class="comment">/* Get Smart Source pre-alloc buff. */</span></div><div class="line">...</div><div class="line"><span class="comment">/* Application puts message data into ubuffer. */</span></div><div class="line">info.flags |= <a class="codeRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a366727d0b5f3039e03b9c4bce8065da5">LBM_SSRC_SEND_EX_FLAG_USER_SUPPLIED_BUFFER</a>;</div><div class="line">info.<a class="codeRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/structlbm__ssrc__send__ex__info__t__stct.html#adcb94e05adb46da7cb8cce93a656b7d4">usr_supplied_buffer</a> = ubuffer;</div><div class="line"><a class="codeRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a2f50f56536778332e9f5712b06546425">lbm_ssrc_send_ex</a>(ssrc, ss_buffer, message_len, 0, &amp;info);</div></div><!-- fragment --><p>Note that the Smart Source pre-allocated buffer <code>ss_buffer</code> also has to be passed in.</p>
<p>Also note that sending messages with the user-supplied message buffer is slightly less CPU efficient than using the pre-allocated buffers. But making pre-allocated buffers larger to accommodate occasional large messages can be very wasteful of memory, depending on the counts of user buffers, transmission window buffers, and retention buffers.</p>
<p><b>UM Fragment Sizes</b></p>
<p>A traditional source will split application messages into "N" fragments when those messages (plus worst-case header) are greater than the <a class="el" href="architecture.html#datagrammaxsizes">Datagram Max Sizes</a>. The size of the first "N-1" fragments will be (approximately) the datagram max size.</p>
<p>With Smart Sources, fragmentation is done somewhat differently. Consider as an example a configuration with a datagram max size of 8192 and a Smart Source max message length of 2000. No UM message fragmentation will happen when the application uses the Smart Source pre-allocated buffers to build outgoing messages. However, if a user-supplied buffer is used, the user can send arbitrarily large application message, and the Smart Source will split the message into "N" fragments. But those fragments will be limited in size to the Smart Source max message length of 2000 bytes of application data (plus additional bytes for headers).</p>
<p>This can lead to unexpected inefficiencies. Continuing the above example, suppose case the application sends a 6000-byte message. The Smart Source will spit it into three 2000-byte datagrams. The underlying IP stack will perform IP fragmentation and send each datagram as two packets of 1500 and 500 bytes respectively, for a total of 6 packets. Whereas if the Smart Source max message length were set to 1500, then the message would be split into 4 fragments of 1500 bytes each, and each fragment would fit in a single packet, for a total of 4 packets. (The calculations above were simplified for clarity, but are not accurate because they do not take into consideration headers.)</p>
<p>When a <a class="el" href="umglossary.html#glossarykernelbypass">kernel bypass</a> network driver is being used, users will sometimes set the datagram max size to approximately an MTU. In that case, it could easily happen that the Smart Source pre-allocated buffers are <em>larger</em> than the datagram max size. In that case, the Smart Source will behave more like a traditional source, splitting the application message into datagrams of (approximately) datagram max size fragments. See <a class="el" href="architecture.html#datagrammaxsizeandnetworkmtu">Datagram Max Size and Network MTU</a>.</p>
<p><br />
 </p>
<h2><a class="anchor" id="smartsourcesandmemorymanagement"></a>
Smart Sources and Memory Management&nbsp;&nbsp;<small><a href="#smartsourcesandmemorymanagement">&lt;-</a></small></h2>
<p>As of UM 6.11, there are new C APIs that give the application greater control over the allocation of memory when Smart Sources are being created. Since creation of a Smart Source pre-allocates buffers used for application message data as well as internal retransmission buffers, an application can override the stock malloc/free to ensure, for example, that memory is local to the CPU core that will be sending messages.</p>
<p>When the application is ready to create the Smart Source, it should set up the configuration option <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#memmgtcallbackssource">mem_mgt_callbacks (source)</a>, which uses the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#aae7c35eda2692944970e73d93217239b">lbm_mem_mgt_callbacks_t</a> structure to specify application callback functions.</p>
<p><br />
 </p>
<h2><a class="anchor" id="smartsourceconfiguration"></a>
Smart Sources Configuration&nbsp;&nbsp;<small><a href="#smartsourceconfiguration">&lt;-</a></small></h2>
<p>The following configuration options are used to control the creation and operation of Smart Sources:</p>
<ul>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a> - should be set to the maximum expected size for messages sent to on the source. </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcuserbuffercountsource">smart_src_user_buffer_count (source)</a> - number of buffers to be pre-created at Smart Source create time. Deleting a Smart Source also frees these buffers, so applications must not access these buffers after their corresponding Smart Source is deleted. </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcretentionbuffercountsource">smart_src_retention_buffer_count (source)</a> - enables <a class="el" href="fundamentalconcepts.html#latejoin">Late Join</a> and <a class="el" href="umfeatures.html#offtransportrecoveryotr">Off-Transport Recovery (OTR)</a> functionality. Takes the place of the normal late join / OTR options "retransmit_retention_*". (On the receive side, the normal late join options apply.) </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#transportlbtrmsmartsrctransmissionwindowbuffercountsource">transport_lbtrm_smart_src_transmission_window_buffer_count (source)</a> - size of the LBT-RM transmission window. Takes the place of the normal window options "transport_lbtrm_transmission_window_*". </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#transportlbtrusmartsrctransmissionwindowbuffercountsource">transport_lbtru_smart_src_transmission_window_buffer_count (source)</a> - size of the LBT-RU transmission window. Takes the place of the normal window options "transport_lbtru_transmission_window_*". </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcenablespectrumchannelsource">smart_src_enable_spectrum_channel (source)</a> - should be set if <a class="el" href="umfeatures.html#spectrum">Spectrum</a> channels will be used. <br />
 See <a class="el" href="umfeatures.html#smartsourcesandspectrum">Smart Sources and Spectrum</a>. </li>
<li>
<a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmessagepropertyintcountsource">smart_src_message_property_int_count (source)</a> - should be set if <a class="el" href="umfeatures.html#messageproperties">Message Properties</a> will be used. <br />
 See <a class="el" href="umfeatures.html#smartsourcesandmessageproperties">Smart Sources and Message Properties</a>. </li>
</ul>
<p>The option <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a> is used to size the window transmission buffers. This means that the first Smart Source created on the session defines the maximum possible size of user messages for all Smart Sources on the Transport Session. It is not legal to create a subsequent Smart Source on the same Transport Session that has a larger <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpsmartsource.html#smartsrcmaxmessagelengthsource">smart_src_max_message_length (source)</a>, although smaller values are permissible.</p>
<p><br />
 </p>
<h2><a class="anchor" id="smartsourcedefensivechecks"></a>
Smart Source Defensive Checks&nbsp;&nbsp;<small><a href="#smartsourcedefensivechecks">&lt;-</a></small></h2>
<p>Ultra Messaging generally includes defensive checks in API functions to verify validity of input parameters. In support of faster operation, deep defensive checks for Smart Sources are optional, and are disabled by default. Users should enable them during application development, and can leave them disabled for production.</p>
<p>To enable deep Smart Source defensive checks, set the environment variable <b>LBM_SMART_SOURCE_CHECK</b> to the numeric sum of desired values. Hexadecimal values may be supplied with the "0x" prefix. Each value enables a class of defensive checking:</p>
<center> <table class="doxtable">
<tr>
<th>Numeric Value</th><th>Deep Check </th></tr>
<tr>
<td><b>1</b></td><td>Send argument checking </td></tr>
<tr>
<td><b>2</b></td><td>Thread checking </td></tr>
<tr>
<td><b>4</b></td><td>User buffer pointer checking </td></tr>
<tr>
<td><b>8</b></td><td>User buffer structure checking </td></tr>
<tr>
<td><b>16, 0x10</b></td><td>user message length checking </td></tr>
<tr>
<td><b>32, 0x20</b></td><td>application header checking, including <a class="el" href="umfeatures.html#spectrum">Spectrum</a> and <a class="el" href="umfeatures.html#messageproperties">Message Properties</a>. </td></tr>
<tr>
<td><b>64, 0x40</b></td><td>null check for User Supplied Buffer (see <a class="el" href="advancedoptimizations.html#smartsourcemessagebuffers">Smart Source Message Buffers</a>) </td></tr>
</table>
</center><p>To enable all checking, set the environment variable <b>LBM_SMART_SOURCE_CHECK</b> to "0xffffffff".</p>
<p><br />
 </p>
<h2><a class="anchor" id="smartsourcerestrictions"></a>
Smart Sources Restrictions&nbsp;&nbsp;<small><a href="#smartsourcerestrictions">&lt;-</a></small></h2>
<ul>
<li>
<p class="startli"><b>Linux and Windows 64-bit Only</b> - Smart Sources is only supported on the 64-bit Linux and 64-bit Windows platforms, C and Java APIs.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>LBT-RM And LBT-RU Sources Only</b> - Smart Sources can only be created with the LBT-RM and LBT-RU transport types. Smart Sources are not compatible with the UM features <a class="el" href="umfeatures.html#multicastimmediatemessaging">Multicast Immediate Messaging</a>, <a class="el" href="umfeatures.html#unicastimmediatemessaging">Unicast Immediate Messaging</a>, or sending responses with <a class="el" href="fundamentalconcepts.html#requestresponse">Request/Response</a>.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Persistence</b> - As of UM 6.11, Smart Sources support <a class="el" href="fundamentalconcepts.html#persistence">Persistence</a>, but with some restrictions. See <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/enablingpersistence.html#smartsourcesandpersistence">Smart Sources and Persistence</a> for details.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Spectrum</b> - As of UM 6.11, Smart Sources support <a class="el" href="umfeatures.html#spectrum">Spectrum</a>, but with some API changes. See <a class="el" href="umfeatures.html#smartsourcesandspectrum">Smart Sources and Spectrum</a> for details.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Single-threaded Only</b> - It is the application's responsibility to serialize calls to Smart Source APIs for a given Transport Session. Concurrent sends to different Transport Sessions are permitted.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No Application Headers</b> - <a class="el" href="umfeatures.html#applicationheaders">Application Headers</a> are not compatible with Smart Sources.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Limited Message Properties</b> - <a class="el" href="umfeatures.html#messageproperties">Message Properties</a> may be included, but their use has restrictions. See <a class="el" href="umfeatures.html#smartsourcemessagepropertiesusage">Smart Source Message Properties Usage</a>.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No Queuing</b> - <a class="el" href="fundamentalconcepts.html#queuing">Queuing</a> is not currently supported, although support for ULB is a possibility in the future.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No Send Request for Java</b> - The Java API does not support sending UM <a class="el" href="fundamentalconcepts.html#requestresponse">Requests</a>. (As of UM version 6.14, the C API does: <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#af20577a5adf4b621a74eb7cb89ad2c16">lbm_ssrc_send_request_ex()</a>).</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No Data Rate Limit</b> - Smart Source data messages are not <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtrmoperation.html#transportlbtrmdataratelimitcontext">rate limited</a>, although retransmissions are <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtrmoperation.html#transportlbtrmretransmitratelimitcontext">rate limited</a>. Care must be taken in designing and provisioning systems to prevent overloading network and host equipment, and overrunning receivers.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>No Hot Failover</b> - Smart Sources are not compatible with <a class="el" href="umfeatures.html#hotfailoverhf">Hot Failover (HF)</a>.</p>
<p class="endli"></p>
</li>
<li>
<b>No Batching</b> - Smart Sources are not compatible with <a class="el" href="architecture.html#implicitbatching">Implicit Batching</a> or <a class="el" href="architecture.html#explicitbatching">Explicit Batching</a>. </li>
</ul>
<dl class="section note"><dt>Note</dt><dd>It is not permitted to mix Smart Source API calls with standard source API calls for a given Transport Session.</dd></dl>
<p><br />
 </p>
<h1><a class="anchor" id="zerocopysendapi"></a>
Zero-Copy Send API&nbsp;&nbsp;<small><a href="#zerocopysendapi">&lt;-</a></small></h1>
<p>This section introduces the use of the zero-copy send API for LBT-RM.</p>
<dl class="section note"><dt>Note</dt><dd>the Zero-Copy Send API feature is <em>not</em> the same thing as the <a class="el" href="advancedoptimizations.html#smartsources">Smart Sources</a> feature; see <a class="el" href="advancedoptimizations.html#comparisonofzerocopyandsmartsources">Comparison of Zero Copy and Smart Sources</a>.</dd></dl>
<p>The zero-copy send API modifies the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a91f4b9cb04fe1323ec56833211cc5cb7">lbm_src_send()</a> function for sending messages such that the UM library does not copy the user's message data before handing the datagram to the socket layer. These changes reduce CPU overhead and provide a minor reduction in latency. The effects are more pronounced for larger user messages, within the restrictions outlined below.</p>
<p>Application code using the zero-copy send API must call <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#adf24bf7c4b07549d6a2f89d56e6c913a">lbm_src_alloc_msg_buff()</a> to request a message buffer into which it will build its outgoing message. That function returns a message buffer pointer and also a separate buffer handle. When the application is ready to send the message, it must call <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a91f4b9cb04fe1323ec56833211cc5cb7">lbm_src_send()</a>, passing the buffer handle as the message (not the message buffer) and specify the LBM_MSG_BUFF_ALLOC send flag.</p>
<p>Once the message is sent, UM will process the buffer asynchronously. Therefore, the application must not make any further reference to either the buffer or the handle.</p>
<p><br />
 </p>
<h2><a class="anchor" id="zerocopysendcompatibility"></a>
Zero-Copy Send Compatibility&nbsp;&nbsp;<small><a href="#zerocopysendcompatibility">&lt;-</a></small></h2>
<p>The zero-copy send API is compatible with the following UM features:</p>
<ul>
<li>
C language, Streaming, source-based publishing applications using LBT-RM. </li>
<li>
Messages sent with the zero-copy API can be received by any UM product or daemon. No special restrictions apply to receivers of messages sent with the zero-copy send API. </li>
<li>
Compatible with implicit batching and message flushing. </li>
<li>
Compatible with non-blocking sends and wakeup source event handling. </li>
<li>
Compatible with hardware timestamps (see section <a class="el" href="umfeatures.html#highresolutiontimestamps">High-resolution Timestamps</a> ). </li>
<li>
Compatible with UD Acceleration. </li>
</ul>
<p><br />
 </p>
<h2><a class="anchor" id="zerocopyrestrictions"></a>
Zero-Copy Restrictions&nbsp;&nbsp;<small><a href="#zerocopyrestrictions">&lt;-</a></small></h2>
<p>Due to the specialized nature of this feature, there are several restrictions in its use:</p>
<ul>
<li>
<b>Languages</b>. Java and .NET are not supported at this time. </li>
<li>
<b>Transport LBT-RM only</b>. Sourced-based LBT-RM (multicast) only. Zero-copy sends are not compatible with LBT-RU, TCP, IPC, SMX, <a class="el" href="umfeatures.html#unicastimmediatemessaging">Unicast Immediate Messaging</a>, or <a class="el" href="umfeatures.html#multicastimmediatemessaging">Multicast Immediate Messaging</a>. Note that an application that uses zero-copy sends for certain sources may also have other sources configured for other transport types. </li>
<li>
<b>Applications only</b>. UM daemons (e.g. <a class="el" href="fundamentalconcepts.html#umrouter">DRO</a>, Stored, etc.) cannot be configured to use the zero-copy API. </li>
<li>
<b>Streaming only</b>. Zero-copy sends are not compatible with <a class="el" href="fundamentalconcepts.html#persistence">Persistence</a> or <a class="el" href="fundamentalconcepts.html#queuing">Queuing</a>. Note that an application that uses zero-copy sends for certain sources may also have other sources mapped to Persistence and/or queuing. </li>
<li>
<b><a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a91f4b9cb04fe1323ec56833211cc5cb7">lbm_src_send()</a> only</b>. zero-copy sends are not compatible with send APIs not supported: <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#af28189e64ef0ee10d3444e418443aaa9">lbm_src_sendv()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a091b5806bf18d10ebd0d9117e0c70229">lbm_src_send_ex()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a4d883eaaa22baf81abf21d495f471c8b">lbm_src_sendv_ex()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#aeff48e558306b4bd869af2d99dcf5f4c">lbm_hf_src_send()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a713c05423f7bf1767a29727c07aa4447">lbm_hf_src_sendv()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ac515c1425d9b3f04f1e2cea5d66d3005">lbm_hf_src_send_ex()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#acb6f42a8811cb998b4b19748b190f39b">lbm_hf_src_sendv_ex()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ad4d06f66b8404684e191ca178e0cc09b">lbm_send_request()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#aabcfb44e7f5188a47d8d56540e783fa7">lbm_send_request_ex()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a0bbc01b600ccc2ae874474e35955eb85">lbm_send_response()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a24e5bff3a70e571bb12024af67b47cbb">lbm_multicast_immediate_message()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a54f3933e4dd154a9c7bb72598d0d9ef1">lbm_multicast_immediate_request()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#a497d77b133cea1547c3346fcba99872a">lbm_unicast_immediate_message()</a>, <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/api.tag:../API/" href="../API/lbm_8h.html#ae011c7a66e1db1f012d7f9633dbd321d">lbm_unicast_immediate_request()</a>. Applications may still use these APIs, but not with the zero-copy send feature. </li>
<li>
<b>Send order</b>. It is recommended that zero-copy buffers be sent in the same order that they are allocated. A future version may require this. </li>
<li>
<a class="el" href="fundamentalconcepts.html#latejoin">Late Join</a>. not compatible with zero-copy sends. Note that an application that uses zero-copy sends on certain sources may also use late join on other sources. </li>
<li>
<a class="el" href="fundamentalconcepts.html#requestresponse">Request/Response</a>. not not compatible with zero-copy sends. </li>
<li>
<a class="el" href="fundamentalconcepts.html#messagemetadata">Message Metadata</a>. Not compatible with <a class="el" href="umfeatures.html#messageproperties">Message Properties</a> or <a class="el" href="umfeatures.html#applicationheaders">Application Headers</a>. Note that an application that uses zero-copy sends for messages without metadata may also send messages with metadata using other send APIs, even to the same source. </li>
<li>
<a class="el" href="umfeatures.html#hotfailoverhf">Hot Failover (HF)</a>. not supported. Note that an application that uses zero-copy sends for certain sources may use hot failover for other sources. </li>
<li>
<a class="el" href="architecture.html#explicitbatching">Explicit Batching</a>. not compatible with zero-copy sends. Note that implicit batching is supported. Also note that an application that uses zero-copy sends for certain sources may use explicit batching for other sources. </li>
<li>
<a class="el" href="architecture.html#messagefragmentationandreassembly">Message Fragmentation and Reassembly</a>. not compatible with zero-copy sends. Messages sent zero-copy must fit within a single datagram, as defined by the LBT-RM <a class="el" href="architecture.html#datagrammaxsizes">Datagram Max Sizes</a>. No special restrictions apply to <a class="el" href="umglossary.html#glossaryipfragmentation">IP fragmentation</a>. Note that an application that uses zero-copy sends for single-datagram messages may also send multi-datagram messages using other send APIs, even to the same source. </li>
</ul>
<p><br />
 </p>
<h1><a class="anchor" id="comparisonofzerocopyandsmartsources"></a>
Comparison of Zero Copy and Smart Sources&nbsp;&nbsp;<small><a href="#comparisonofzerocopyandsmartsources">&lt;-</a></small></h1>
<p>There are two UM features that are intended to reduce latency and <a class="el" href="umglossary.html#glossaryjitter">jitter</a> when sending messages: </p><ul>
<li>
<a class="el" href="advancedoptimizations.html#smartsources">Smart Sources</a> </li>
<li>
<a class="el" href="advancedoptimizations.html#zerocopysendapi">Zero-Copy Send API</a> </li>
</ul>
<p>These two features use different approaches to latency and jitter reduction, and are not compatible with each other. There are trade offs explained below, and users seeking latency and/or jitter reduction will sometimes need to try both and empirically measure which is better for their use case.</p>
<p>The zero-copy send API removes a copy of the user's data buffer, as compared to a normal send. For small messages of a few hundred bytes, a malloc and a data copy represent a very small amount of time, so unless your messages are large, the absolute latency reduction is minimal.</p>
<p>The Smart Source has the advantage of eliminating all mallocs and frees from the send path. In addition, all thread locking is eliminated. This essentially removes all sources of jitter from the UM send path. Also, the Smart Source feature supports <a class="el" href="umglossary.html#glossaryumfragmentation">UM fragmentation</a>, which zero-copy sends do not. However, because of the approach taken, sending to a Smart Source is somewhat more restrictive than sending with the zero-copy API.</p>
<p>In general, Informatica recommends Smart Sources to achieve the maximum reduction in jitter. For example, the zero-copy send API supports the use of batching to combine multiple messages into a single network datagram. Batching can be essential to achieve high throughputs. Some application designers may determine that the throughput advantages of zero-copy with batching outweigh the jitter advantages of Smart Sources.</p>
<p>See the sections <a class="el" href="advancedoptimizations.html#zerocopysendapi">Zero-Copy Send API</a> and <a class="el" href="advancedoptimizations.html#smartsources">Smart Sources</a> for details of their restrictions.</p>
<p><br />
 </p>
<h1><a class="anchor" id="xsplatencyreduction"></a>
XSP Latency Reduction&nbsp;&nbsp;<small><a href="#xsplatencyreduction">&lt;-</a></small></h1>
<p>A common source of latency outliers is when Topic Resolution packets are received at the same time that user data messages are received. The UM context thread might process those Topic Resolution packets before processing the user data messages.</p>
<p>By using the XSP feature, user data reception can be moved to a different thread than topic resolution reception. See <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a> for details, paying careful attention to <a class="el" href="umfeatures.html#xspthreadingconsiderations">XSP Threading Considerations</a>.</p>
<p><br />
 </p>
<h1><a class="anchor" id="receivesidebatching"></a>
Receive-Side Batching&nbsp;&nbsp;<small><a href="#receivesidebatching">&lt;-</a></small></h1>
<p>The receive-side batching feature can improve throughput of subscribers that use UM <a class="el" href="umobjects.html#eventqueueobject">event queues</a> and/or are written in Java.</p>
<p>See also source-side <a class="el" href="architecture.html#messagebatching">Message Batching</a>.</p>
<p>UM <a class="el" href="umobjects.html#eventqueueobject">event queues</a> introduce a small overhead which adds latency and reduces maximum sustainable throughput. UM's Java API also introduces a small overhead to deliver received messages from the UM library. Both of these overheads can usually be mitigated with the receive-side batching feature.</p>
<p>This feature is enabled via the configuration option <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/config.tag:../Config/" href="../Config/grpdeliverycontrol.html#deliverycontrolmessagebatchingcontext">delivery_control_message_batching (context)</a>.</p>
<p>The feature works by collecting receiver events into bundles. For an event queue, the bundle is sent to the dispatch thread as a single queue item. For Java, the bundle is passed to the Java language as a single object. In both cases, the overhead is amortized across the multiple messages in the bundle.</p>
<p>This feature is most effective when used along with the <a class="el" href="advancedoptimizations.html#receivemultipledatagrams">Receive Multiple Datagrams</a> feature (Linux only), which will increase the bundle size automatically if the receiver falls behind. A publisher using <a class="el" href="architecture.html#messagebatching">Message Batching</a> also increases the bundle size. Larger bundles means greater efficiency gains.</p>
<dl class="section warning"><dt>Warning</dt><dd>Do not use receive-side batching for C or .NET programs that are not using event queues. The feature does not increase efficiency and can actually reduce it.</dd>
<dd>
This feature is not compatible with <a class="el" href="umfeatures.html#transportservicesproviderxsp">XSP</a>. Users of XSP are typically trying to maximize the performance of UM, and probably should not be using <a class="el" href="umobjects.html#eventqueueobject">event queues</a>. For alternatives, please <a href="https://ultramessaging.github.io/UM_Support.html">contact Informatica Support</a>.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>If you enable receive-side batching and you use an event queue that is in polling mode, using lbm_event_dispatch(evq, LBM_EVENT_QUEUE_POLL), then rather than dispatching exactly one event per call to lbm_event_dispatch, you may get multiple events dispatched with a single call.</dd></dl>
<p><br />
 </p>
<h1><a class="anchor" id="corepinning"></a>
Core Pinning&nbsp;&nbsp;<small><a href="#corepinning">&lt;-</a></small></h1>
<p>The Unix and Windows operating systems attempt to balance CPU utilization across all available CPU cores. They often do this without regard to the architectural design of the system hardware, which can introduce significant inefficiencies. For example, if a thread's execution migrates from one <a href="https://queue.acm.org/detail.cfm?id=2513149">NUMA node</a> to another, the code will frequently need to access memory located in the other NUMA zone, which happens over a slower memory interconnect.</p>
<p>Fortunately, Unix and Windows support <a class="el" href="umglossary.html#glossarypinning">pinning</a> processes and threads to specific CPU cores. It is the user's responsibility to understand the host architecture sufficiently to know which cores are grouped into NUMA zones. Pinning a group of related threads to cores within the same NUMA zone is important to maintain high performance.</p>
<p>However, even letting the operating system migrate a thread from one core to another within a single NUMA zone has the side effect of invalidating the cache, which introduces latency. You get the best performance when each thread is pinned to its own core, with no other threads contending for that core. This approach obviously severely limits the number of threads that can run on a host.</p>
<p>UM does not have a general feature that pins threads to cores for applications. It is the user's responsibility to pin (set affinity) using the appropriate operating system APIs.</p>
<p>The Persistent Store allows the user to assign individual threads to specific cores. See <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.16/doc/Design/ume.tag:../UME/" href="../UME/storethreadaffinity.html">Store Thread Affinity</a> for details.</p>
<p>For the <a class="el" href="fundamentalconcepts.html#umrouter">DRO</a>, it is not possible to identify specific threads and assign them to individual cores. But the user can use the operating system's user interface to assign the entire DRO process to a group of cores known to be in the same NUMA zone.</p>
<p><br />
 </p>
<h1><a class="anchor" id="memorylatencyreduction"></a>
Memory Latency Reduction&nbsp;&nbsp;<small><a href="#memorylatencyreduction">&lt;-</a></small></h1>
<p>UM makes use of dynamic memory allocation/deallocation using malloc() and free(). The default memory allocator included with Linux and Windows can sometimes introduce latency outliers of multiple milliseconds. It is rare, but we have seen outliers as long as 10 milliseconds.</p>
<p>There are higher-performing allocators available, many of them open-source. For example, <a href="http://hoard.org/">Hoard</a>. There are <a href="https://en.wikipedia.org/wiki/C_dynamic_memory_allocation#Implementations">many others</a>.</p>
<p>A good commercial product is <a href="http://microquill.com/smartheap/">MicroQuill's SmartHeap</a>. In fact, the Persistent Store is built and ships with SmartHeap. Note that licensing Ultra Messaging does not grant a license to the customer for general use of SmartHeap. Users who want to use SmartHeap in applications should contact <a href="http://microquill.com/">MicroQuill</a> directly.</p>
<p>None of these products can <b>guarantee</b> that there will never be millisecond-long latencies, but they can greatly reduce the frequency.</p>
<p><br />
<br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
</p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
</body>
</html>
