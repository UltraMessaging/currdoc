<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Concepts Guide: Packet Loss</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen_manual.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Concepts Guide
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('packetloss.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Packet Loss </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This section is about packet loss. It applies primarily to UDP-based network protocols used by UM: LBT-RM, LBT-RU, and UDP-based topic resolution.</p>
<p>(Packet loss also affects TCP-based protocols, but UM has no control over how it is handled by the IP stack. "Packet" loss can also happen with the IPC transport type if source-pacing is selected, but most of this section's discussion doesn't apply to IPC.)</p>
<p>Packet loss is almost always caused when some part of the system is receiving packets at a higher rate than it is able to process them. This might be a router, a switch, a subscribing host's kernel, or the subscribing process itself. This typically results in queuing of incoming packets within the router, switch, kernel, or socket buffer. But queues do not have unlimited size. If the incoming packets exceed the processing speed for too long a period of time, the queue will fill and packets will be dropped.</p>
<p>Packet loss is a fact of life in networks. Some users are able to provision and tune their systems such that they might only lose a few packets per week. Other users routinely live with several lost packets per minute. Many users do not monitor their system for loss and have no idea how frequent it is.</p>
<p>Packet loss is undesirable for many reasons. For reliable protocols (TCP, LBT-RM, etc), detection and retransmission of lost packets introduces significant latency. If packet rates are too high for too long a period of time, the transport protocol can give up trying to recover the lost data. For <a class="el" href="fundamentalconcepts.html#streaming">Streaming</a>, this can cause disconnects or <a class="el" href="fundamentalconcepts.html#unrecoverableloss1">Unrecoverable Loss</a>, where application messages can be lost forever.</p>
<p>Informatica recommends that you: </p><ol>
<li>
Design your system to prevent packet loss. </li>
<li>
Configure UM for proper recovery from packet loss. </li>
<li>
Monitor your network for packet loss. </li>
<li>
Diagnose the cause of the observed packet loss and fine-tune your design and configuration to better prevent loss and optimize recovery from it. </li>
</ol>
<p><br />
 </p>
<h1><a class="anchor" id="designtopreventloss"></a>
Design to Prevent Loss&nbsp;&nbsp;<small><a href="#designtopreventloss">&lt;-</a></small></h1>
<p>There are two complementary methods of avoiding packet loss: </p><ul>
<li>
<a class="el" href="packetloss.html#decreasepacketflowthroughlosspoints">Decrease Packet Flow through Loss Points</a>. </li>
<li>
<a class="el" href="packetloss.html#increaseefficiencyofpacketconsumers">Increase Efficiency of Packet Consumers</a>. </li>
</ul>
<p><br />
 </p>
<h2><a class="anchor" id="decreasepacketflowthroughlosspoints"></a>
Decrease Packet Flow through Loss Points&nbsp;&nbsp;<small><a href="#decreasepacketflowthroughlosspoints">&lt;-</a></small></h2>
<ul>
<li>
<p class="startli"><b><a class="el" href="architecture.html#messagebatching">Message Batching</a></b>. At most loss points, the number of packets is usually more important than the sizes of the packets. 100 packets of 60 bytes each is <em>much</em> more burdensome to packet consumers than 10 packets of 600 bytes each. For latency-sensitive applications, consider implementing an <a class="el" href="architecture.html#intelligentbatching">Intelligent Batching</a> algorithm.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Reduce discards</b>. Due to the way publishers map topics to <a class="el" href="fundamentalconcepts.html#transportsessions">Transport Sessions</a>, it is often the case that the receiver will have to discard messages that it hasn't subscribed to. The <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/api.tag:../API/" href="../API/structlbm__rcv__transport__stats__t__stct.html">LBT-RM transport statistics</a> structure for each transport type contains the field "lbm_msgs_no_topic_rcved" which counts the number of data messages discarded. Also, the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/api.tag:../API/" href="../API/structlbm__context__stats__t__stct.html">context statistics</a> structure contains the field "lbtrm_unknown_msgs_rcved", which also counts data messages discarded.</p>
<p>For the LBT-RU transport type, you can get similar discards. See the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/api.tag:../API/" href="../API/structlbm__rcv__transport__stats__lbtru__t__stct.html">LBT-RU transport statistics</a> structure field "lbm_msgs_no_topic_rcved", and the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/api.tag:../API/" href="../API/structlbm__context__stats__t__stct.html">context statistics</a> structure field "lbtru_unknown_msgs_rcved".</p>
<p>For the TCP or LBT-RU transport types, you can often decrease discards by turning on <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/config.tag:../Config/" href="../Config/grpmajoroptions.html#transportsourcesidefilteringbehaviorsource">Source Side Filtering</a>.</p>
<p class="endli">With Multicast, the Source Side Filtering feature is not possible. So it is usually necessary to change the topic-to-transport session mapping. Ideally, this can be done by taking into account the topic interests of the subscribers. But often it simply means increasing the number of transport sessions. In the case of LBT-RM, increasing the number of multicast groups is preferred, but is often limited by the network hardware. In that case, you can multiply the number of transport sessions by varying the destination port. </p>
</li>
</ul>
<p><br />
 </p>
<h2><a class="anchor" id="increaseefficiencyofpacketconsumers"></a>
Increase Efficiency of Packet Consumers&nbsp;&nbsp;<small><a href="#increaseefficiencyofpacketconsumers">&lt;-</a></small></h2>
<p>Here are some methods for increasing the efficiency of subscribers: Use a <a class="el" href="umglossary.html#glossarykernelbypass">kernel-bypass driver</a>. For Linux, use <a class="el" href="advancedoptimizations.html#receivemultipledatagrams">Receive Multiple Datagrams</a>. Use <a class="el" href="advancedoptimizations.html#receivebufferrecycling">Receive Buffer Recycling</a>. For Java and .NET, use <a class="el" href="advancedoptimizations.html#zeroobjectdelivery">Zero Object Delivery</a>.</p>
<p><br />
 </p>
<h1><a class="anchor" id="umrecoveryoflostpackets"></a>
UM Recovery of Lost Packets&nbsp;&nbsp;<small><a href="#umrecoveryoflostpackets">&lt;-</a></small></h1>
<p>See <a class="el" href="fundamentalconcepts.html#messagingreliability">Messaging Reliability</a> for a high-level description of message loss as it relates to UM.</p>
<p>UM recovers lost packets at multiple levels: </p><ul>
<li>
<b>Transport</b> - TCP, LBT-RU, LBT-RM have low-level handshakes to detect and retransmit lost packets. </li>
<li>
<b>OTR/Late Join</b> - independent of transport, OTR and Late Join will recover data, typically after the transport layer is unable to recover. </li>
<li>
<b>Persistence</b> - closely-associated with OTR and Late Join, the persistent Store provides a much greater capacity to recover lost data. UM persistence provides a guarantee of message delivery, subject to a set of configurable constraints. </li>
</ul>
<p>One common goal for most UM use cases is that users want the flow of new messages to continue unimpeded in parallel with recovery efforts of lost packets. Given that packet loss is almost always a result of high packet rates overloading one or more queuing points along a messaging path, the addition of packet recovery efforts can make the overload even worse. "Pouring gasoline on a fire" is an often-repeated metaphor.</p>
<p>Fortunately, packet rate overload tends to be temporary, associated with short-term traffic bursts. That is one reason why the UM lost packet recovery algorithms use time delays. For example: <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtrmreliability.html#transportlbtrmnakinitialbackoffintervalreceiver">transport_lbtrm_nak_initial_backoff_interval (receiver)</a> and <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/config.tag:../Config/" href="../Config/grpofftransportrecovery.html#otrrequestinitialdelayreceiver">otr_request_initial_delay (receiver)</a>. By waiting before requesting retransmissions, the burst is allowed some time to subside before we add retransmission to the normal traffic load. These delays do add to latency, but shortening the delay too much risks making the loss worse, which can make the overall latency worse than having a longer delay.</p>
<p>One limiting factor related to data recovery is UM's use of retransmission rate limits. After a short period of severe packet loss due to overload, many receivers will be requesting retransmission. It would make no sense for the initial request delay to successfully bypass the original traffic burst, only to create its own overloading burst of retransmissions. Older NAK-based systems can get into a positive feedback loop where loss leads to retransmission, which leads to more loss, which leads to more retransmission, etc. Even once new data rates return to normal, networks can be locked into this kind of NAK/Retransmission storm. UM's retransmission rate limiter throttles the retransmission of lost packets over time without worsening the loss.</p>
<p>Another limiting factor related to data recovery is the amount of data which the sender buffers and is available for retransmission. Applications need to continue sending new data while recovery takes place. Since the buffer is of limited size, older buffered messages will eventually be overwritten with new messages.</p>
<p>For streaming applications, these buffers are held in memory, and the sizes are usually measured in megabytes. For persistent applications, the Store writes its buffer to disk, allowing for buffer sizes orders of magnitude larger than memory-based buffers. But even the Store's disk-based buffer is of finite size, and is susceptible to being overwritten if it takes too long to recover data.</p>
<p>Note that the <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/ume.tag:../UME/" href="../UME/operationalview.html#rppnormaloperation">Receiver Paced Persistence (RPP)</a> feature seeks to maximize the reliability of messaging by allowing the publisher to be blocked from sending rather than overwriting unacknowledged data.</p>
<p>Given these limiting factors for data recovery, a sufficiently-overloaded network can reach a point where lost data can no longer be recovered, a situation called "unrecoverable loss".</p>
<p>Finally, it is very important for UM users to make use of UM's extensive monitoring capabilities. Since UM does a good job of recovering lost packets, you may be experiencing high latency spikes without knowing it. And recovered loss today can be a warning of unrecoverable loss tomorrow. User are strongly advised to monitor transport statistics and pay special attention to receivers that repeatedly experience loss. Even if that loss is successfully recovered, you should diagnose and treat the loss before it gets worse and becomes unrecoverable.</p>
<p>See <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/operations.tag:../Operations/" href="../Operations/monitoring.html">Monitoring</a> for more information.</p>
<p><br />
 </p>
<h1><a class="anchor" id="packetlosspoints"></a>
Packet Loss Points&nbsp;&nbsp;<small><a href="#packetlosspoints">&lt;-</a></small></h1>
<p>There are just a few common points at which packets are normally lost:</p>
<div class="image">
<img src="loss_points.png" alt="loss_points.png"/>
</div>
 <p>The red buffers/queues are the most common locations where packets are typically lost during a packet burst.</p>
<p><br />
 </p>
<h2><a class="anchor" id="lossswitchegressport"></a>
Loss: Switch Egress Port&nbsp;&nbsp;<small><a href="#lossswitchegressport">&lt;-</a></small></h2>
<p>The switch egress port can come under pressure if data flows from multiple sources need to be merged onto a single outgoing link. The outgoing link can be an internal trunk or communication link connecting two pieces of network equipment, but more typically it is link to a destination host.</p>
<p>It is easy to understand how loss can happen here. Suppose three remote hosts are sending UDP data streams at the destination host. If each stream is carrying 0.5 gigabit/sec of throughput, the switch needs to send 1.5 gigabit/sec over a 1 gigabit link, a clear overload. If this is a very short-term burst, the egress queue will hold the data until the incoming data flows subside and the outgoing port can get caught up. But if the burst lasts too long and the egress queue fills, the switch has no choice but to drop packets.</p>
<p>Note that the switch will not count these drops as "errors". There is a separate drop counter which should be examined to diagnose switch egress port loss.</p>
<p><a class="anchor" id="mitigateswitchegressport"></a><b>MITIGATION</b></p>
<p>The only solution is to reduce the packet rate being sent to the destination host. See <a class="el" href="packetloss.html#decreasepacketflowthroughlosspoints">Decrease Packet Flow through Loss Points</a>.</p>
<p><br />
 </p>
<h2><a class="anchor" id="lossnicringbuffer"></a>
Loss: NIC Ring Buffer&nbsp;&nbsp;<small><a href="#lossnicringbuffer">&lt;-</a></small></h2>
<p>As packets are received by the host's NIC (Network Interface Card), they are copied into host memory in a structure called the Receive Ring Buffer. The NIC interrupts the OS, which has the responsibility to unload the packet buffers from the Ring Buffer. If the incoming packet rate is faster than the OS can unload the Ring Buffer, it will fill and packets will be dropped.</p>
<p>Normally, the kernel is able to service NIC interrupts without any trouble. However, there is one situation which can put the Ring Buffer under pressure: When multiple processes on the host are subscribed to the same multicast stream, the kernel must replicate and deliver the packets to each process. For a small number of processes (5-10), the kernel will still be able to keep up with the incoming packets.</p>
<p>However, as companies consolidation servers by moving to large, many-core hosts (often virtualized), we see the same multicast stream subscribed to by increasing numbers of processes on the same physical server. We have seen NIC Ring Buffer loss (also called "overrun") with as few as 15 processes subscribed to a heavy stream of multicast packets.</p>
<p>(Note that this is generally only a problem for multicast. With Unicast data distribution to many recipients, the source essentially does the packet replication work. The receive-side work for the kernel for each unicast packet is minimal.)</p>
<p><a class="anchor" id="mitigatenicringbuffer"></a><b>MITIGATION</b></p>
<p>Users should maximize the size of the NIC's Receive Ring Buffer. For many NICs, the size of the ring buffer is configured by the number of receive descriptors. This should be set to the maximum allowable value.</p>
<p>The mitigators listed in <a class="el" href="packetloss.html#lossswitchegressport">Loss: Switch Egress Port</a> will also help this problem by reducing the incoming packet rate.</p>
<p>Another solution is to spread processes across more physical hosts. This has the additional advantage of reducing latency, since multicast replication within a host must be done in software by the kernel and is serial in nature, whereas replication in the network is done by specialized hardware in parallel.</p>
<p>Another possible solution involves the use of the DRO as the primary receiver of the multicast data, which then re-publishes it on the host using the IPC transport. This has the disadvantage of introducing some additional latency since the messages must be received by the DRO and then forwarded to the application receivers. It also requires separating the applications to their own <a class="el" href="fundamentalconcepts.html#topicresolutiondomain">Topic Resolution Domains</a> (TRDs).</p>
<p><br />
 </p>
<h2><a class="anchor" id="losssocketbuffer"></a>
Loss: Socket Buffer&nbsp;&nbsp;<small><a href="#losssocketbuffer">&lt;-</a></small></h2>
<p>The Socket Buffer represents the interface between the OS kernel and the user process. Received data is transferred from the NIC Ring Buffer to the destination Socket Buffer(s). The Socket Buffers are then emptied by the application process (in this case, the UM Context Thread). Socket buffer sizes are configurable, according to the transport type. For example, see <a class="elRef" doxygen="/29W/Amun/home/jenkins/backup_exclude.areas/builds/UMQ_6.15/doc/Design/config.tag:../Config/" href="../Config/grptransportlbtrmreliability.html#transportlbtrmreceiversocketbuffercontext">transport_lbtrm_receiver_socket_buffer (context)</a>.</p>
<p>The TCP protocol is designed to ensure that the socket buffer cannot be overflowed. However, UDP-based protocols (LBT-RU and LBT-RM) are susceptible to socket buffer overflow, which leads to datagram loss.</p>
<p><a class="anchor" id="mitigatenicsocketbuffer"></a><b>MITIGATION</b></p>
<p>All of the mitigators listed <a class="el" href="packetloss.html#lossnicringbuffer">Loss: NIC Ring Buffer</a> will help this problem by reducing the incoming packet rate.</p>
<p>An obvious mitigator is to increase the sizes of the receive socket buffers. Informatica usually recommends at least 8MB for UDP-based protocols. But this only works if the problem is related to short-term traffic bursts. Simply increasing the size of the buffer will not avoid loss if the <em>average</em> message rate exceeds the average consumption and processing rate of the receiving program.</p>
<p>A very useful method for mitigating socket buffer loss is to increase the efficiency of the receiving application. The <a class="el" href="advancedoptimizations.html#receivemultipledatagrams">Receive Multiple Datagrams</a> can increase that efficiency without sacrificing latency.</p>
<p>Also, the <a class="el" href="umfeatures.html#transportservicesproviderxsp">Transport Services Provider (XSP)</a> feature can help by splitting the work of unloading multiple sockets across multiple threads.</p>
<p><br />
 </p>
<h2><a class="anchor" id="lossother"></a>
Loss: Other&nbsp;&nbsp;<small><a href="#lossother">&lt;-</a></small></h2>
<p>The three loss locations described above are all related to high packet rates causing fixed-sized packet buffers to overflow. These represent by far the most common reasons for packet loss. However, it is possible that you will experience loss that cannot be diagnosed to those three causes.</p>
<p>For example, we have seen reports of NIC hardware malfunctioning such that most packets are successfully received and delivered, but some percentage of packets fail. At least one user reported that a misconfigured router "flapped" a route, resulting in periodic, short-term loss of connectivity between two sub-networks. We have seen a case where the use of kernel bypass drivers for high-performance NICs (specifically Solarflare) can cause multicast deafness if both accelerated and non-accelerated processes are run on the same host. We have even seen a case where replacing the Ethernet cable between a host and the switch resolved packet loss.</p>
<p>It is not possible to have a step-by-step diagnostic procedure which will pinpoint every possible cause of packet loss. The techniques described in this document should successfully diagnose a large majority of packet loss causes, but nothing can replace your infrastructure network engineers expertise at tracking down problems.</p>
<p><br />
 </p>
<h1><a class="anchor" id="verifyinglossdetectiontools"></a>
Verifying Loss Detection Tools&nbsp;&nbsp;<small><a href="#verifyinglossdetectiontools">&lt;-</a></small></h1>
<p>The preceding techniques for mitigating loss are best deployed after you have identified the type of loss. Unfortunately, we have found that the tools available to detect and identify the location of loss to be problematic. Informatica does not provide such tools, and does not follow the market for such tools to find a reliable supplier.</p>
<p>However, we have a starting point that has given us some measure of success in diagnosing the loss points. It is important that you try out these tools to verify that they properly detect the different types of loss. In order to verify them, you need to be able to reproduce on demand loss at each of the points: switch, NIC, and socket buffer.</p>
<p>Fortunately, this is reasonably easy using the <code>msend</code> and <code>mdump</code> tools provided in the "mtools" package offered by Informatica free of charge. Download the mtools package from <a href="https://community.informatica.com/solutions/informatica_mtools">https://community.informatica.com/solutions/informatica_mtools</a> The source files for <code>msend</code> and <code>mdump</code> are provided, as well as pre-built binaries for most major platforms.</p>
<p>Informatica recommends verifying your loss diagnosis tools <em>before</em> you have a serious loss event that disrupts your application system, preferably before your system goes into full production usage. Periodically running and recording the results of these tools during normal operation will make it possible to diagnose loss after the fact. Detecting and identifying non-severe (recoverable) loss can be used to prevent serious (unrecoverable) loss events in the future.</p>
<p><br />
 </p>
<h2><a class="anchor" id="preparetoverify"></a>
Prepare to Verify&nbsp;&nbsp;<small><a href="#preparetoverify">&lt;-</a></small></h2>
<ol>
<li>
Download and install mtools on two hosts, designated "sender" and "receiver". Informatica recommends that the hosts be "bare metal" (not virtual machines), and that they be connected to the same switch. This minimizes the chances that the verification tests will cause any disruption to normal operation. </li>
<li>
Contact your system and network administrators and set up some time that they can work with your during the verification process. They will need to perform operations that you probably do not have the ability to do. </li>
<li>
Have the network administrator allocate a multicast group that you can use for this test. That multicast group should be otherwise unused in your organization. Warn the administrator that you will be pushing intense traffic bursts between the two hosts. </li>
</ol>
<p><br />
 </p>
<h2><a class="anchor" id="verifyingswitchloss"></a>
Verifying Switch Loss&nbsp;&nbsp;<small><a href="#verifyingswitchloss">&lt;-</a></small></h2>
<p>A possible Unix command that a network administrator could use is:</p>
<p><code>snmpwalk -v 1 -c public</code> <em>SWITCH_ADDR</em> <code>IF-MIB</code><code>::</code><code>ifOutDiscards</code></p>
<p>Note that the above community string ("public") is probably not enabled; the network administrator will know the appropriate value. Ideally, the network administrator would run that command every 5 or 10 minutes, logging to a file, with a time stamp. If this log file could be shared read-only to the project groups, they can time-correlate any unusual application event with loss reported by the switch.</p>
<p>To verify that you properly detect switch loss, follow these steps:</p>
<ol>
<li>
Work with your system and network administrators to <b>enable</b> Ethernet flow control in both the switch port and the NIC. </li>
<li>
Use the above <code>snmpwalk</code> command (or equivalent) to record the current drop counts for the switch ports. </li>
<li>
On the receiving host, run 30 copies of the following command: <br />
<code>mdump -q</code> <em>MCAST_ADDR</em> <code>12000</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the receiving host. </li>
<li>
On the sending host, run the following command: <br />
<code>msend -5</code> <em>MCAST_ADDR</em> <code>12000 15</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the sending host. </li>
<li>
When the test completes, use the <code>snmpwalk</code> command again (or equivalent) to record another set of drop counters. The receiving host's drop count should be larger. </li>
</ol>
<p>This test works by making the receiving host's kernel work very hard for each received datagram. It should be unable to keep up. (If you don't see any drops caused by the test, try doubling the number of copies of <code>mdump</code> on the receiving host.) The Ethernet flow control settings on the NIC and switch will prevent NIC loss in its ring buffer by slowing down the switch's egress port. Thus, the switch's egress queue will fill and should overflow.</p>
<p><br />
 </p>
<h2><a class="anchor" id="verifyingnicloss"></a>
Verifying NIC Loss&nbsp;&nbsp;<small><a href="#verifyingnicloss">&lt;-</a></small></h2>
<p><b>Unix</b></p>
<p>On some Unix systems, the "ifconfig" command will accurately report receive overrun on the NIC. For example: <br />
<code>ifconfig eth0</code></p>
<p>But in many Unix systems, the values reported by "ifconfig" remain at zero, even when the NIC has in fact overrun its receive ring buffer. We recommend also trying the "ethtool" command. For example: <br />
<code>ethtool -s eth0</code></p>
<p><b>Windows</b></p>
<p>To the best of our knowledge, there is no standard Windows tool for detecting NIC loss. Some drivers might provide that information from the interface control panel. Otherwise, you might need to download a management application from the NIC or system vendor.</p>
<p>If you know of a widely-available method to detect NIC overrun on Windows, please let us know at our <b>DLMessagingBuilds</b> email account on informatica.com (that awkward wording used to avoid spam address harvesters).</p>
<p>To verify that you properly detect NIC loss, follow these steps:</p>
<ol>
<li>
Work with your system and network administrators to <b>disable</b> Ethernet flow control in both the switch port and the NIC. </li>
<li>
Use your NIC loss tool to get the current receive overrun count. </li>
<li>
On the receiving host, run 30 copies of the following command: <br />
<code>mdump -q</code> <em>MCAST_ADDR</em> <code>12000</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the receiving host. </li>
<li>
On the sending host, run the following command: <br />
<code>msend -5</code> <em>MCAST_ADDR</em> <code>12000 15</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the sending host. </li>
<li>
When the test completes, use the NIC loss tool again to record the receive overrun count. </li>
</ol>
<p>This test works by making the receiving host's kernel work very hard for each received datagram. It should be unable to keep up. (If you don't see any drops caused by the test, try doubling the number of copies of <code>mdump</code> on the receiving host.) The lack of Ethernet flow control means that the switch will send the packets at full line rate, which should overflow the NIC ring buffer.</p>
<p><br />
 </p>
<h2><a class="anchor" id="verifyingsocketbufferloss"></a>
Verifying Socket Buffer Loss&nbsp;&nbsp;<small><a href="#verifyingsocketbufferloss">&lt;-</a></small></h2>
<p>On most systems, the <code>netstat</code> command can be used to detect socket buffer overflow. For example: <br />
<code>netstat -s</code> <br />
Look in the UDP section for "receive errors". This normally represents the number of datagrams dropped due to the receive socket buffer being full.</p>
<p>Note that Windows prior to version 7 does not increment that field for socket buffer overflows. If you have pre-Windows 7, we don't know of any command to detect socket buffer overflow.</p>
<p>To verify that you properly detect socket buffer overflow, follow these steps:</p>
<ol>
<li>
Use <code>netstat -s</code> to get the current receive error count. </li>
<li>
On the receiving host, run a single copy of the command: <br />
<code>mdump -q -p1000/5</code> <em>MCAST_ADDR</em> <code>12000</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the receiving host. </li>
<li>
On the sending host, run the following command: <br />
<code>msend -5 -s2200</code> <em>MCAST_ADDR</em> <code>12000 15</code> <em>INTFC_ADDR</em> <br />
where <em>MCAST_ADDR</em> is the multicast group for the test, and <em>INTFC_ADDR</em> is the IP address of the sending host. </li>
<li>
When the test completes, use <code>netstat -s</code> again to get the new receive error count. </li>
</ol>
<p>This test works by introducing a short sleep in the "mdump" command between reception of datagrams. This causes the socket buffer to overflow.</p>
<p><br />
<br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
 <br />
</p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
</body>
</html>
